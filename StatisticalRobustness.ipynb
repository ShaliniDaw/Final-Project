{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d09e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded per_track_features.csv\n",
      "Shape: (4263, 60)\n",
      "Columns: 60\n",
      "\n",
      "Genotype counts:\n",
      "Genotype\n",
      "WT     2815\n",
      "MUT    1448\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique stacks: 16\n",
      "Unique tracks: 4263\n",
      "\n",
      "Top 15 columns by % missing:\n",
      "                     column  pct_missing\n",
      "0   directional_consistency        67.00\n",
      "1      temporal_persistence        67.00\n",
      "2                   alpha_x        58.64\n",
      "3                     P_est        58.64\n",
      "4                     alpha        58.64\n",
      "5                   P_est_y        58.64\n",
      "6                   D_est_y        58.64\n",
      "7                       K_y        58.64\n",
      "8                   alpha_y        58.64\n",
      "9                   P_est_x        58.64\n",
      "10                    D_est        58.64\n",
      "11                        K        58.64\n",
      "12                 speed_cv        58.64\n",
      "13         area_trend_slope        58.64\n",
      "14  early_eccentricity_mean        58.64\n"
     ]
    }
   ],
   "source": [
    "# Block 1: Setup + Load + sanity checks\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"/Users/asus/Downloads/MSc Final Project Stuff/outputs_final/per_track_features.csv\")\n",
    "OUT_DIR = Path(\"/Users/asus/Downloads/MSc Final Project Stuff/outputs_addendum\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_df(df: pd.DataFrame, name: str) -> Path:\n",
    "    \"\"\"Save a dataframe to the addendum output directory.\"\"\"\n",
    "    out_path = OUT_DIR / f\"{name}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    return out_path\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Loaded per_track_features.csv\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", len(df.columns))\n",
    "\n",
    "# Expected identifiers\n",
    "expected_id_cols = {\"stack\", \"track_id\", \"Genotype\"}\n",
    "missing = expected_id_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected identifier columns: {missing}\")\n",
    "\n",
    "print(\"\\nGenotype counts:\")\n",
    "print(df[\"Genotype\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\nUnique stacks:\", df[\"stack\"].nunique())\n",
    "print(\"Unique tracks:\", df[\"track_id\"].nunique())\n",
    "\n",
    "# Basic NA overview\n",
    "na_summary = (df.isna().mean().sort_values(ascending=False) * 100).round(2)\n",
    "na_summary_df = na_summary.reset_index()\n",
    "na_summary_df.columns = [\"column\", \"pct_missing\"]\n",
    "save_df(na_summary_df, \"missingness_percent_by_column\")\n",
    "\n",
    "print(\"\\nTop 15 columns by % missing:\")\n",
    "print(na_summary_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab74b9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: 58\n",
      "Metric columns (numeric, excluding IDs): 57\n",
      "Saved metric manifest.\n"
     ]
    }
   ],
   "source": [
    "# Block 2: Define testable metric columns\n",
    "\n",
    "ID_COLS = [\"stack\", \"track_id\", \"Genotype\"]\n",
    "\n",
    "# Choose numeric columns only (robust to accidental strings)\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# \"Testable metrics\" are numeric columns that aren't identifiers\n",
    "metric_cols = [c for c in numeric_cols if c not in ID_COLS]\n",
    "\n",
    "if not metric_cols:\n",
    "    raise ValueError(\"No numeric metric columns found. Check CSV content/types.\")\n",
    "\n",
    "print(\"Numeric columns:\", len(numeric_cols))\n",
    "print(\"Metric columns (numeric, excluding IDs):\", len(metric_cols))\n",
    "\n",
    "# Creating a metadata table of columns to be used\n",
    "metric_manifest = pd.DataFrame({\n",
    "    \"metric\": metric_cols,\n",
    "    \"dtype\": [str(df[m].dtype) for m in metric_cols],\n",
    "    \"n_missing\": [int(df[m].isna().sum()) for m in metric_cols],\n",
    "    \"pct_missing\": [(df[m].isna().mean() * 100) for m in metric_cols],\n",
    "})\n",
    "metric_manifest[\"pct_missing\"] = metric_manifest[\"pct_missing\"].round(2)\n",
    "\n",
    "save_df(metric_manifest.sort_values(\"pct_missing\", ascending=False), \"metric_manifest\")\n",
    "print(\"Saved metric manifest.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6526d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw distribution diagnostics.\n",
      "\n",
      "Most skewed metrics (top 10):\n",
      "               metric  skewness  pct_zero        median            max\n",
      "10            D_est_x   18.2728      0.00  1.458014e+01  285807.923987\n",
      "14            D_est_y   18.2728      0.00  1.458014e+01  285807.923987\n",
      "18              D_est   18.2728      0.00  1.458014e+01  285807.923987\n",
      "1   total_path_length    8.2591     44.12  2.914317e+00    5366.956233\n",
      "15            P_est_y    7.0455      0.00  1.705679e-07    5242.156716\n",
      "11            P_est_x    7.0455      0.00  1.705679e-07    5242.156716\n",
      "19              P_est    7.0455      0.00  1.705679e-07    5242.156716\n",
      "0          num_frames    5.7225      0.00  2.000000e+00     210.000000\n",
      "42  speed_trend_slope    3.7814      0.00  7.243178e-01      36.241242\n",
      "17                  K    3.1928      0.00  1.340062e+02    3530.995913\n"
     ]
    }
   ],
   "source": [
    "# Block 3: Distribution diagnostics (raw)\n",
    "\n",
    "def compute_distribution_diagnostics(data: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        s = data[c]\n",
    "        s_nonnull = s.dropna()\n",
    "        n = len(s)\n",
    "        n_nonnull = len(s_nonnull)\n",
    "\n",
    "        # If everything is missing, record and move on\n",
    "        if n_nonnull == 0:\n",
    "            rows.append({\n",
    "                \"metric\": c,\n",
    "                \"n\": n,\n",
    "                \"n_nonnull\": 0,\n",
    "                \"pct_missing\": 100.0,\n",
    "                \"pct_zero\": np.nan,\n",
    "                \"min\": np.nan,\n",
    "                \"p25\": np.nan,\n",
    "                \"median\": np.nan,\n",
    "                \"p75\": np.nan,\n",
    "                \"max\": np.nan,\n",
    "                \"skewness\": np.nan,\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        pct_missing = 100.0 * (1 - (n_nonnull / n))\n",
    "        pct_zero = 100.0 * (s_nonnull.eq(0).mean())\n",
    "\n",
    "        # Use pandas skew (Fisher-Pearson)\n",
    "        skewness = float(s_nonnull.skew())\n",
    "\n",
    "        rows.append({\n",
    "            \"metric\": c,\n",
    "            \"n\": n,\n",
    "            \"n_nonnull\": n_nonnull,\n",
    "            \"pct_missing\": round(pct_missing, 2),\n",
    "            \"pct_zero\": round(pct_zero, 2),\n",
    "            \"min\": float(s_nonnull.min()),\n",
    "            \"p25\": float(s_nonnull.quantile(0.25)),\n",
    "            \"median\": float(s_nonnull.median()),\n",
    "            \"p75\": float(s_nonnull.quantile(0.75)),\n",
    "            \"max\": float(s_nonnull.max()),\n",
    "            \"skewness\": round(skewness, 4),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "diag_raw = compute_distribution_diagnostics(df, metric_cols)\n",
    "save_df(diag_raw.sort_values(\"skewness\", ascending=False), \"distribution_diagnostics_raw\")\n",
    "\n",
    "print(\"Saved raw distribution diagnostics.\")\n",
    "print(\"\\nMost skewed metrics (top 10):\")\n",
    "print(diag_raw.sort_values(\"skewness\", ascending=False).head(10)[[\"metric\", \"skewness\", \"pct_zero\", \"median\", \"max\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd21ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform plan saved. log1p applied to 38 metrics.\n",
      "Saved per_track_features_with_transforms.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 4: Transformations (create new columns, keep raw intact)\n",
    "\n",
    "def choose_transform_for_metric(series: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Decide transformation based on support:\n",
    "    - If all values are >= 0 (ignoring NaNs), use log1p.\n",
    "    - Else, use 'none' (leave as-is) for this addendum.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    if len(s) == 0:\n",
    "        return \"none\"\n",
    "    if (s >= 0).all():\n",
    "        return \"log1p\"\n",
    "    return \"none\"\n",
    "\n",
    "transform_plan = []\n",
    "df_t = df.copy()\n",
    "\n",
    "for c in metric_cols:\n",
    "    plan = choose_transform_for_metric(df[c])\n",
    "    transform_plan.append({\"metric\": c, \"transform\": plan})\n",
    "\n",
    "    if plan == \"log1p\":\n",
    "        # log1p is safe at zero; keeps scale reasonable for heavy-tailed positive metrics\n",
    "        df_t[f\"{c}__log1p\"] = np.log1p(df[c])\n",
    "    else:\n",
    "        # We do not auto-transform metrics that can be negative in this minimal addendum.\n",
    "        pass\n",
    "\n",
    "transform_plan_df = pd.DataFrame(transform_plan)\n",
    "save_df(transform_plan_df, \"transform_plan\")\n",
    "\n",
    "# Quick counts\n",
    "n_log = int((transform_plan_df[\"transform\"] == \"log1p\").sum())\n",
    "print(f\"Transform plan saved. log1p applied to {n_log} metrics.\")\n",
    "\n",
    "# Save transformed dataset\n",
    "save_df(df_t[[*ID_COLS, *[c for c in df_t.columns if c in metric_cols or c.endswith('__log1p')]]],\n",
    "        \"per_track_features_with_transforms\")\n",
    "print(\"Saved per_track_features_with_transforms.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9acf433e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features before missingness filter: 95\n",
      "Keeping (<= 70% missing): 95\n",
      "Dropping (> 70% missing): 0\n",
      "\n",
      "After collapsing _x/_y duplicates:\n",
      "Model table shape: (4263, 90)\n",
      "Final feature count: 87\n",
      "Saved: \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\model_table_ready.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 5: Build clean modeling table (post-transform) + handle duplicates + define X/y/groups\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "USE_SAVED = True\n",
    "\n",
    "TRANSFORMED_PATH = OUT_DIR / \"per_track_features_with_transforms.csv\"\n",
    "\n",
    "if USE_SAVED:\n",
    "    df_transformed = pd.read_csv(TRANSFORMED_PATH)\n",
    "\n",
    "# --- Safety checks ---\n",
    "required_cols = {\"stack\", \"track_id\", \"Genotype\"}\n",
    "missing_req = required_cols - set(df_transformed.columns)\n",
    "if missing_req:\n",
    "    raise ValueError(f\"Block 5 expected columns missing: {missing_req}\")\n",
    "\n",
    "# Standardize label -> binary (MUT=1, WT=0)\n",
    "y = (df_transformed[\"Genotype\"].astype(str).str.upper() == \"MUT\").astype(int).values\n",
    "groups = df_transformed[\"stack\"].astype(str).values  # group-aware CV to avoid stack leakage\n",
    "\n",
    "# --- Step 1: choose numeric feature columns (exclude identifiers) ---\n",
    "id_cols = {\"stack\", \"track_id\", \"Genotype\"}\n",
    "numeric_cols = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [c for c in numeric_cols if c not in id_cols]\n",
    "\n",
    "# --- Step 2: drop extremely-missing columns (keep threshold explicit) ---\n",
    "# Rationale: many columns are >58% missing; beyond ~60–70% the model becomes imputation-driven.\n",
    "missing_pct = df_transformed[feature_cols].isna().mean() * 100\n",
    "\n",
    "DROP_MISSING_OVER = 70.0\n",
    "kept_cols = missing_pct[missing_pct <= DROP_MISSING_OVER].index.tolist()\n",
    "dropped_cols = missing_pct[missing_pct > DROP_MISSING_OVER].index.tolist()\n",
    "\n",
    "print(f\"Features before missingness filter: {len(feature_cols)}\")\n",
    "print(f\"Keeping (<= {DROP_MISSING_OVER:.0f}% missing): {len(kept_cols)}\")\n",
    "print(f\"Dropping (> {DROP_MISSING_OVER:.0f}% missing): {len(dropped_cols)}\")\n",
    "\n",
    "# --- Step 3: handle “_x/_y” duplicated columns from merges ---\n",
    "# If both alpha_x and alpha_y (etc.) are present, often one is redundant.\n",
    "# Strategy: if both exist, create a single column as:\n",
    "#   - prefer non-null from _x, else _y\n",
    "# and then drop the suffix columns.\n",
    "def collapse_xy_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cols = df.columns\n",
    "    x_cols = [c for c in cols if c.endswith(\"_x\")]\n",
    "    for cx in x_cols:\n",
    "        base = cx[:-2]  # remove \"_x\"\n",
    "        cy = base + \"_y\"\n",
    "        if cy in cols:\n",
    "            # create collapsed column only if base not already present\n",
    "            if base not in cols:\n",
    "                df[base] = df[cx].combine_first(df[cy])\n",
    "            else:\n",
    "                # if base exists, fill base from x/y where base is null\n",
    "                df[base] = df[base].combine_first(df[cx]).combine_first(df[cy])\n",
    "\n",
    "            # drop the suffix columns\n",
    "            df.drop(columns=[cx, cy], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_model = df_transformed[[\"stack\", \"track_id\", \"Genotype\"] + kept_cols].copy()\n",
    "df_model = collapse_xy_columns(df_model)\n",
    "\n",
    "# Recompute feature columns after collapsing\n",
    "numeric_cols2 = df_model.select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols2 = [c for c in numeric_cols2 if c not in id_cols]\n",
    "\n",
    "# --- Step 4: quick sanity report ---\n",
    "print(\"\\nAfter collapsing _x/_y duplicates:\")\n",
    "print(\"Model table shape:\", df_model.shape)\n",
    "print(\"Final feature count:\", len(feature_cols2))\n",
    "\n",
    "# Save the modeling-ready table for downstream blocks\n",
    "model_path = OUT_DIR / \"model_table_ready.csv\"\n",
    "df_model.to_csv(model_path, index=False)\n",
    "print(f\"Saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990a4d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model_table_ready.csv\n",
      "X shape: (4263, 87) | #features: 87\n",
      "y counts: {0: 2815, 1: 1448}\n",
      "Unique stacks: 16\n",
      "\n",
      "CV comparison summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cv</th>\n",
       "      <th>acc_mean</th>\n",
       "      <th>acc_std</th>\n",
       "      <th>bal_acc_mean</th>\n",
       "      <th>bal_acc_std</th>\n",
       "      <th>roc_auc_mean</th>\n",
       "      <th>roc_auc_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StratifiedKFold</td>\n",
       "      <td>0.6803</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.7808</td>\n",
       "      <td>0.0118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GroupKFold_by_stack</td>\n",
       "      <td>0.2867</td>\n",
       "      <td>0.2726</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cv  acc_mean  acc_std  bal_acc_mean  bal_acc_std  \\\n",
       "0      StratifiedKFold    0.6803   0.0158        0.7227       0.0135   \n",
       "1  GroupKFold_by_stack    0.2867   0.2726        0.2944       0.2885   \n",
       "\n",
       "   roc_auc_mean  roc_auc_std  \n",
       "0        0.7808       0.0118  \n",
       "1           NaN          NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\cv_baseline_logreg_summary.csv\n",
      "Saved: \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\cv_baseline_logreg_folds.csv\n"
     ]
    }
   ],
   "source": [
    "# Block 6: Leakage-safe baseline modeling (GroupKFold vs StratifiedKFold) with robust metrics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Paths \n",
    "MODEL_PATH = OUT_DIR / \"model_table_ready.csv\"\n",
    "\n",
    "df_model = pd.read_csv(MODEL_PATH)\n",
    "\n",
    "# Identify columns\n",
    "id_cols = {\"stack\", \"track_id\", \"Genotype\"}\n",
    "feature_cols = [c for c in df_model.columns if c not in id_cols and np.issubdtype(df_model[c].dtype, np.number)]\n",
    "\n",
    "X = df_model[feature_cols]\n",
    "y = (df_model[\"Genotype\"].astype(str).str.upper() == \"MUT\").astype(int).values\n",
    "groups = df_model[\"stack\"].astype(str).values\n",
    "\n",
    "print(\"Loaded model_table_ready.csv\")\n",
    "print(\"X shape:\", X.shape, \"| #features:\", len(feature_cols))\n",
    "print(\"y counts:\", pd.Series(y).value_counts().to_dict())\n",
    "print(\"Unique stacks:\", len(np.unique(groups)))\n",
    "\n",
    "# Pipeline: impute -> scale -> logistic regression\n",
    "pipe = Pipeline(steps=[\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"liblinear\", class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "# Scorers\n",
    "scoring = {\n",
    "    \"acc\": \"accuracy\",\n",
    "    \"bal_acc\": make_scorer(balanced_accuracy_score),\n",
    "    \"roc_auc\": \"roc_auc\"\n",
    "}\n",
    "\n",
    "# CV strategies\n",
    "cv_strat = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_group = GroupKFold(n_splits=min(5, len(np.unique(groups))))\n",
    "\n",
    "def summarize_cv(name, cv, use_groups=False):\n",
    "    res = cross_validate(\n",
    "        pipe,\n",
    "        X, y,\n",
    "        cv=cv.split(X, y, groups=groups) if use_groups else cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    summary = {\n",
    "        \"cv\": name,\n",
    "        \"acc_mean\": float(np.mean(res[\"test_acc\"])),\n",
    "        \"acc_std\":  float(np.std(res[\"test_acc\"])),\n",
    "        \"bal_acc_mean\": float(np.mean(res[\"test_bal_acc\"])),\n",
    "        \"bal_acc_std\":  float(np.std(res[\"test_bal_acc\"])),\n",
    "        \"roc_auc_mean\": float(np.mean(res[\"test_roc_auc\"])),\n",
    "        \"roc_auc_std\":  float(np.std(res[\"test_roc_auc\"])),\n",
    "    }\n",
    "    return summary, res\n",
    "\n",
    "summaries = []\n",
    "raw_results = {}\n",
    "\n",
    "s1, r1 = summarize_cv(\"StratifiedKFold\", cv_strat, use_groups=False)\n",
    "summaries.append(s1); raw_results[\"StratifiedKFold\"] = r1\n",
    "\n",
    "s2, r2 = summarize_cv(\"GroupKFold_by_stack\", cv_group, use_groups=True)\n",
    "summaries.append(s2); raw_results[\"GroupKFold_by_stack\"] = r2\n",
    "\n",
    "summary_df = pd.DataFrame(summaries).round(4)\n",
    "print(\"\\nCV comparison summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Save outputs\n",
    "summary_path = OUT_DIR / \"cv_baseline_logreg_summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"Saved: {summary_path}\")\n",
    "\n",
    "# Also save fold-level results for transparency\n",
    "fold_rows = []\n",
    "for cv_name, res in raw_results.items():\n",
    "    n = len(res[\"test_acc\"])\n",
    "    for i in range(n):\n",
    "        fold_rows.append({\n",
    "            \"cv\": cv_name,\n",
    "            \"fold\": i,\n",
    "            \"acc\": res[\"test_acc\"][i],\n",
    "            \"bal_acc\": res[\"test_bal_acc\"][i],\n",
    "            \"roc_auc\": res[\"test_roc_auc\"][i],\n",
    "        })\n",
    "\n",
    "fold_df = pd.DataFrame(fold_rows).round(6)\n",
    "fold_path = OUT_DIR / \"cv_baseline_logreg_folds.csv\"\n",
    "fold_df.to_csv(fold_path, index=False)\n",
    "print(f\"Saved: {fold_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3542fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model_table_ready.csv\n",
      "X shape: (4263, 87) | #features: 87\n",
      "y counts: {0: np.int64(2815), 1: np.int64(1448)}\n",
      "Unique stacks: 16\n",
      "\n",
      "Saved:\n",
      "- \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\perm_importance_groupkfold_fold_scores.csv\n",
      "- \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\permutation_importance_groupkfold_raw.csv\n",
      "- \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\permutation_importance_groupkfold_summary.csv\n",
      "\n",
      "Top 20 features by mean drop in balanced accuracy (higher = more important):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean_delta</th>\n",
       "      <th>std_delta</th>\n",
       "      <th>n_folds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D_est</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P_est</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>area_delta_mean</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>early_speed_mean</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>r_peak__log1p</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>n_stationary__log1p</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>n_stationary</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>r_peak</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>temporal_persistence</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>early_area_mean__log1p</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>late_area_mean</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>K</td>\n",
       "      <td>-0.000800</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>area_trend_slope</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>orientation_delta_mean</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K_x__log1p</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K_y__log1p</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>K__log1p</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>total_path_length</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>orientation_slope</td>\n",
       "      <td>-0.001804</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>late_area_mean__log1p</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  mean_delta  std_delta  n_folds\n",
       "0                    D_est    0.000810   0.001001        3\n",
       "8                    P_est    0.000705   0.000990        3\n",
       "15         area_delta_mean    0.000451   0.000925        3\n",
       "32        early_speed_mean    0.000414   0.001049        3\n",
       "73           r_peak__log1p    0.000000   0.000000        3\n",
       "54     n_stationary__log1p    0.000000   0.000000        3\n",
       "53            n_stationary    0.000000   0.000000        3\n",
       "72                  r_peak    0.000000   0.000000        3\n",
       "84    temporal_persistence   -0.000068   0.000753        3\n",
       "29  early_area_mean__log1p   -0.000535   0.000649        3\n",
       "43          late_area_mean   -0.000671   0.002220        3\n",
       "4                        K   -0.000800   0.004292        3\n",
       "19        area_trend_slope   -0.001101   0.001195        3\n",
       "61  orientation_delta_mean   -0.001214   0.001893        3\n",
       "6               K_x__log1p   -0.001431   0.001960        3\n",
       "7               K_y__log1p   -0.001431   0.001960        3\n",
       "5                 K__log1p   -0.001431   0.001960        3\n",
       "85       total_path_length   -0.001549   0.001008        3\n",
       "63       orientation_slope   -0.001804   0.002123        3\n",
       "44   late_area_mean__log1p   -0.001995   0.002507        3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 7: Leakage-aware permutation importance (GroupKFold by stack)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Inputs assumed from prior blocks ---\n",
    "# model_df: DataFrame that includes [\"stack\",\"Genotype\"] + feature columns (already transformed)\n",
    "# feature_cols: list of final feature columns used for modelling\n",
    "# OUT_DIR: Path to output folder\n",
    "\n",
    "model_df = pd.read_csv(OUT_DIR / \"model_table_ready.csv\")\n",
    "\n",
    "# Build X, y, groups\n",
    "X = model_df[feature_cols].copy()\n",
    "y = (model_df[\"Genotype\"] == \"MUT\").astype(int).values\n",
    "groups = model_df[\"stack\"].values\n",
    "\n",
    "print(\"Loaded model_table_ready.csv\")\n",
    "print(\"X shape:\", X.shape, \"| #features:\", X.shape[1])\n",
    "print(\"y counts:\", dict(pd.Series(y).value_counts()))\n",
    "print(\"Unique stacks:\", pd.Series(groups).nunique())\n",
    "\n",
    "# --- Model ---\n",
    "# Reason: permutation importance is easiest to interpret with a stable baseline classifier.\n",
    "clf = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"model\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "# --- GroupKFold (stack-level separation) ---\n",
    "n_splits = min(5, pd.Series(groups).nunique())\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "importance_rows = []\n",
    "fold_scores = []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups=groups), start=1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # If a fold ends up single-class in train or test (possible in edge cases), skip safely.\n",
    "    if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:\n",
    "        fold_scores.append({\"fold\": fold_idx, \"balanced_acc\": np.nan, \"skipped\": True})\n",
    "        continue\n",
    "\n",
    "    # Fit + evaluate\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    fold_scores.append({\"fold\": fold_idx, \"balanced_acc\": bal_acc, \"skipped\": False})\n",
    "\n",
    "    # Permutation importance on held-out fold\n",
    "    perm = permutation_importance(\n",
    "        clf,\n",
    "        X_test, y_test,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_repeats=20,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    for feat, mean_imp, std_imp in zip(feature_cols, perm.importances_mean, perm.importances_std):\n",
    "        importance_rows.append({\n",
    "            \"fold\": fold_idx,\n",
    "            \"feature\": feat,\n",
    "            \"delta_bal_acc_mean\": float(mean_imp),\n",
    "            \"delta_bal_acc_std\": float(std_imp),\n",
    "            \"n_test\": int(len(test_idx)),\n",
    "            \"test_stacks\": int(pd.Series(groups[test_idx]).nunique()),\n",
    "        })\n",
    "\n",
    "importance_df = pd.DataFrame(importance_rows)\n",
    "fold_scores_df = pd.DataFrame(fold_scores)\n",
    "\n",
    "# Save fold performance \n",
    "fold_scores_df.to_csv(OUT_DIR / \"perm_importance_groupkfold_fold_scores.csv\", index=False)\n",
    "\n",
    "if importance_df.empty:\n",
    "    raise ValueError(\n",
    "        \"Permutation importance still produced no rows. \"\n",
    "        \"This usually means every GroupKFold split had a single-class train/test set. \"\n",
    "        \"If so, dataset may be genotype-separated by stack with no mixing across stacks.\"\n",
    "    )\n",
    "\n",
    "# Aggregate across folds: stability summary\n",
    "importance_summary = (\n",
    "    importance_df\n",
    "    .groupby(\"feature\", as_index=False)\n",
    "    .agg(\n",
    "        mean_delta=(\"delta_bal_acc_mean\", \"mean\"),\n",
    "        std_delta=(\"delta_bal_acc_mean\", \"std\"),\n",
    "        n_folds=(\"fold\", \"nunique\")\n",
    "    )\n",
    "    .sort_values(\"mean_delta\", ascending=False)\n",
    ")\n",
    "\n",
    "importance_df.to_csv(OUT_DIR / \"permutation_importance_groupkfold_raw.csv\", index=False)\n",
    "importance_summary.to_csv(OUT_DIR / \"permutation_importance_groupkfold_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"-\", OUT_DIR / \"perm_importance_groupkfold_fold_scores.csv\")\n",
    "print(\"-\", OUT_DIR / \"permutation_importance_groupkfold_raw.csv\")\n",
    "print(\"-\", OUT_DIR / \"permutation_importance_groupkfold_summary.csv\")\n",
    "\n",
    "print(\"\\nTop 20 features by mean drop in balanced accuracy (higher = more important):\")\n",
    "display(importance_summary.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03261af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\method_sanity_summary.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>section</th>\n",
       "      <th>metric</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data</td>\n",
       "      <td>n_rows</td>\n",
       "      <td>4263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>n_features</td>\n",
       "      <td>87.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data</td>\n",
       "      <td>n_stacks</td>\n",
       "      <td>16.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>pct_missing_overall_features</td>\n",
       "      <td>29.1190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transforms</td>\n",
       "      <td>n_log1p_features</td>\n",
       "      <td>38.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>labels</td>\n",
       "      <td>count_WT</td>\n",
       "      <td>2815.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>labels</td>\n",
       "      <td>count_MUT</td>\n",
       "      <td>1448.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cv</td>\n",
       "      <td>0_acc_mean</td>\n",
       "      <td>0.6803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cv</td>\n",
       "      <td>0_bal_acc_mean</td>\n",
       "      <td>0.7227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cv</td>\n",
       "      <td>0_acc_std</td>\n",
       "      <td>0.0158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cv</td>\n",
       "      <td>0_bal_acc_std</td>\n",
       "      <td>0.0135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cv</td>\n",
       "      <td>1_acc_mean</td>\n",
       "      <td>0.2867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cv</td>\n",
       "      <td>1_bal_acc_mean</td>\n",
       "      <td>0.2944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>cv</td>\n",
       "      <td>1_acc_std</td>\n",
       "      <td>0.2726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cv</td>\n",
       "      <td>1_bal_acc_std</td>\n",
       "      <td>0.2885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       section                        metric      value\n",
       "0         data                        n_rows  4263.0000\n",
       "1         data                    n_features    87.0000\n",
       "2         data                      n_stacks    16.0000\n",
       "3         data  pct_missing_overall_features    29.1190\n",
       "4   transforms              n_log1p_features    38.0000\n",
       "5       labels                      count_WT  2815.0000\n",
       "6       labels                     count_MUT  1448.0000\n",
       "7           cv                    0_acc_mean     0.6803\n",
       "8           cv                0_bal_acc_mean     0.7227\n",
       "9           cv                     0_acc_std     0.0158\n",
       "10          cv                 0_bal_acc_std     0.0135\n",
       "11          cv                    1_acc_mean     0.2867\n",
       "12          cv                1_bal_acc_mean     0.2944\n",
       "13          cv                     1_acc_std     0.2726\n",
       "14          cv                 1_bal_acc_std     0.2885"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 8: Method sanity summary\n",
    "# Handles CV summaries where the CV strategy is stored as the index\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_TABLE_PATH = OUT_DIR / \"model_table_ready.csv\"\n",
    "CV_SUMMARY_PATH  = OUT_DIR / \"cv_baseline_logreg_summary.csv\"\n",
    "\n",
    "# Load inputs\n",
    "model_df = pd.read_csv(MODEL_TABLE_PATH)\n",
    "cv_summary = pd.read_csv(CV_SUMMARY_PATH)\n",
    "\n",
    "# Ensure identifier columns exist\n",
    "id_cols = [\"stack\", \"track_id\", \"Genotype\"]\n",
    "missing_ids = [c for c in id_cols if c not in model_df.columns]\n",
    "if missing_ids:\n",
    "    raise ValueError(f\"model_table_ready.csv missing required columns: {missing_ids}\")\n",
    "\n",
    "# Identify numeric feature columns\n",
    "feature_cols = [\n",
    "    c for c in model_df.columns\n",
    "    if c not in id_cols and pd.api.types.is_numeric_dtype(model_df[c])\n",
    "]\n",
    "\n",
    "# Basic dataset stats\n",
    "n_rows     = len(model_df)\n",
    "n_features = len(feature_cols)\n",
    "n_stacks   = model_df[\"stack\"].nunique()\n",
    "geno_counts = model_df[\"Genotype\"].value_counts(dropna=False).to_dict()\n",
    "\n",
    "# Transform footprint\n",
    "n_log1p = sum(c.endswith(\"_log1p\") for c in feature_cols)\n",
    "\n",
    "# Overall missingness\n",
    "pct_missing_overall = float(model_df[feature_cols].isna().mean().mean() * 100)\n",
    "\n",
    "# ---- CV SUMMARY ----\n",
    "# If CV strategy is not a column, assume it's the index\n",
    "if \"cv_strategy\" not in cv_summary.columns:\n",
    "    cv_summary = cv_summary.reset_index().rename(columns={\"index\": \"cv_strategy\"})\n",
    "\n",
    "required_cv_cols = {\n",
    "    \"cv_strategy\",\n",
    "    \"acc_mean\",\n",
    "    \"acc_std\",\n",
    "    \"bal_acc_mean\",\n",
    "    \"bal_acc_std\"\n",
    "}\n",
    "missing_cv = required_cv_cols - set(cv_summary.columns)\n",
    "if missing_cv:\n",
    "    raise ValueError(f\"CV summary missing required columns: {sorted(missing_cv)}\")\n",
    "\n",
    "# Build compact method summary\n",
    "summary_rows = []\n",
    "\n",
    "# Dataset summary\n",
    "summary_rows.extend([\n",
    "    {\"section\": \"data\", \"metric\": \"n_rows\", \"value\": n_rows},\n",
    "    {\"section\": \"data\", \"metric\": \"n_features\", \"value\": n_features},\n",
    "    {\"section\": \"data\", \"metric\": \"n_stacks\", \"value\": n_stacks},\n",
    "    {\"section\": \"data\", \"metric\": \"pct_missing_overall_features\", \"value\": round(pct_missing_overall, 3)},\n",
    "    {\"section\": \"transforms\", \"metric\": \"n_log1p_features\", \"value\": n_log1p},\n",
    "])\n",
    "\n",
    "# Label balance\n",
    "for k, v in geno_counts.items():\n",
    "    summary_rows.append({\n",
    "        \"section\": \"labels\",\n",
    "        \"metric\": f\"count_{k}\",\n",
    "        \"value\": int(v)\n",
    "    })\n",
    "\n",
    "# CV performance summary\n",
    "for _, r in cv_summary.iterrows():\n",
    "    summary_rows.extend([\n",
    "        {\"section\": \"cv\", \"metric\": f\"{r['cv_strategy']}_acc_mean\", \"value\": float(r[\"acc_mean\"])},\n",
    "        {\"section\": \"cv\", \"metric\": f\"{r['cv_strategy']}_bal_acc_mean\", \"value\": float(r[\"bal_acc_mean\"])},\n",
    "        {\"section\": \"cv\", \"metric\": f\"{r['cv_strategy']}_acc_std\", \"value\": float(r[\"acc_std\"])},\n",
    "        {\"section\": \"cv\", \"metric\": f\"{r['cv_strategy']}_bal_acc_std\", \"value\": float(r[\"bal_acc_std\"])},\n",
    "    ])\n",
    "\n",
    "method_summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# Save\n",
    "out_path = OUT_DIR / \"method_sanity_summary.csv\"\n",
    "method_summary_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "display(method_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0ca50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\perm_importance_with_stability.csv\n",
      "\n",
      "Top 20 features (with stability tag):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean_delta</th>\n",
       "      <th>std_delta</th>\n",
       "      <th>n_folds</th>\n",
       "      <th>stability_score</th>\n",
       "      <th>stability_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D_est</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>3</td>\n",
       "      <td>0.808956</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_est</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>3</td>\n",
       "      <td>0.711502</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>area_delta_mean</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>3</td>\n",
       "      <td>0.487622</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>early_speed_mean</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>3</td>\n",
       "      <td>0.395047</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r_peak__log1p</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n_stationary__log1p</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n_stationary</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>r_peak</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>temporal_persistence</td>\n",
       "      <td>-0.000068</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>3</td>\n",
       "      <td>0.089837</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>early_area_mean__log1p</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>0.000649</td>\n",
       "      <td>3</td>\n",
       "      <td>0.823784</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>late_area_mean</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>3</td>\n",
       "      <td>0.302125</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>K</td>\n",
       "      <td>-0.000800</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>3</td>\n",
       "      <td>0.186424</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>area_trend_slope</td>\n",
       "      <td>-0.001101</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>3</td>\n",
       "      <td>0.920669</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>orientation_delta_mean</td>\n",
       "      <td>-0.001214</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>3</td>\n",
       "      <td>0.641096</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>K_x__log1p</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>3</td>\n",
       "      <td>0.729784</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>K_y__log1p</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>3</td>\n",
       "      <td>0.729784</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>K__log1p</td>\n",
       "      <td>-0.001431</td>\n",
       "      <td>0.001960</td>\n",
       "      <td>3</td>\n",
       "      <td>0.729784</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>total_path_length</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>3</td>\n",
       "      <td>1.535908</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>orientation_slope</td>\n",
       "      <td>-0.001804</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>3</td>\n",
       "      <td>0.849731</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>late_area_mean__log1p</td>\n",
       "      <td>-0.001995</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>3</td>\n",
       "      <td>0.795595</td>\n",
       "      <td>weak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  mean_delta  std_delta  n_folds  stability_score  \\\n",
       "0                    D_est    0.000810   0.001001        3         0.808956   \n",
       "1                    P_est    0.000705   0.000990        3         0.711502   \n",
       "2          area_delta_mean    0.000451   0.000925        3         0.487622   \n",
       "3         early_speed_mean    0.000414   0.001049        3         0.395047   \n",
       "4            r_peak__log1p    0.000000   0.000000        3         0.000000   \n",
       "5      n_stationary__log1p    0.000000   0.000000        3         0.000000   \n",
       "6             n_stationary    0.000000   0.000000        3         0.000000   \n",
       "7                   r_peak    0.000000   0.000000        3         0.000000   \n",
       "8     temporal_persistence   -0.000068   0.000753        3         0.089837   \n",
       "9   early_area_mean__log1p   -0.000535   0.000649        3         0.823784   \n",
       "10          late_area_mean   -0.000671   0.002220        3         0.302125   \n",
       "11                       K   -0.000800   0.004292        3         0.186424   \n",
       "12        area_trend_slope   -0.001101   0.001195        3         0.920669   \n",
       "13  orientation_delta_mean   -0.001214   0.001893        3         0.641096   \n",
       "14              K_x__log1p   -0.001431   0.001960        3         0.729784   \n",
       "15              K_y__log1p   -0.001431   0.001960        3         0.729784   \n",
       "16                K__log1p   -0.001431   0.001960        3         0.729784   \n",
       "17       total_path_length   -0.001549   0.001008        3         1.535908   \n",
       "18       orientation_slope   -0.001804   0.002123        3         0.849731   \n",
       "19   late_area_mean__log1p   -0.001995   0.002507        3         0.795595   \n",
       "\n",
       "   stability_tag  \n",
       "0           weak  \n",
       "1           weak  \n",
       "2           weak  \n",
       "3           weak  \n",
       "4           weak  \n",
       "5           weak  \n",
       "6           weak  \n",
       "7           weak  \n",
       "8           weak  \n",
       "9           weak  \n",
       "10          weak  \n",
       "11          weak  \n",
       "12          weak  \n",
       "13          weak  \n",
       "14          weak  \n",
       "15          weak  \n",
       "16          weak  \n",
       "17          weak  \n",
       "18          weak  \n",
       "19          weak  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 9: Stability flags for permutation importance \n",
    "# - Uses the GroupKFold permutation importance summary.\n",
    "# - Adds simple stability tags based on magnitude vs variability.\n",
    "# - Exports \"perm_importance_with_stability.csv\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PERM_SUMMARY_PATH = OUT_DIR / \"permutation_importance_groupkfold_summary.csv\"\n",
    "\n",
    "imp = pd.read_csv(PERM_SUMMARY_PATH)\n",
    "\n",
    "# Validate expected columns\n",
    "required_cols = {\"feature\", \"mean_delta\", \"std_delta\", \"n_folds\"}\n",
    "missing = required_cols - set(imp.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Permutation importance summary missing columns: {sorted(missing)}\")\n",
    "\n",
    "# Ensure numeric types\n",
    "for c in [\"mean_delta\", \"std_delta\", \"n_folds\"]:\n",
    "    imp[c] = pd.to_numeric(imp[c], errors=\"coerce\")\n",
    "\n",
    "# Define a small epsilon to avoid division issues\n",
    "eps = 1e-12\n",
    "\n",
    "# Signal-to-noise style score (higher = more stable relative to noise)\n",
    "imp[\"stability_score\"] = imp[\"mean_delta\"].abs() / (imp[\"std_delta\"].abs() + eps)\n",
    "\n",
    "# Simple tags \n",
    "# - \"stable\": consistently non-trivial and not wildly variable\n",
    "# - \"weak\": tiny effect\n",
    "# - \"unstable\": high variability relative to effect size\n",
    "abs_mean = imp[\"mean_delta\"].abs()\n",
    "\n",
    "# Thresholds\n",
    "weak_thr = abs_mean.quantile(0.50)  # median effect size\n",
    "stable_thr = abs_mean.quantile(0.80)\n",
    "\n",
    "imp[\"stability_tag\"] = \"unstable\"\n",
    "imp.loc[abs_mean < weak_thr, \"stability_tag\"] = \"weak\"\n",
    "imp.loc[(abs_mean >= stable_thr) & (imp[\"stability_score\"] >= 1.0), \"stability_tag\"] = \"stable\"\n",
    "\n",
    "# Sort by importance magnitude\n",
    "imp = imp.sort_values(\"mean_delta\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "out_path = OUT_DIR / \"perm_importance_with_stability.csv\"\n",
    "imp.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"\\nTop 20 features (with stability tag):\")\n",
    "display(imp.head(20)[[\"feature\", \"mean_delta\", \"std_delta\", \"n_folds\", \"stability_score\", \"stability_tag\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aafba777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: \\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\final_addendum_summary.csv\n",
      "\n",
      "CV (tidy) preview:\n",
      "           cv_strategy  acc_mean  acc_std  bal_acc_mean  bal_acc_std  \\\n",
      "0      StratifiedKFold    0.6803   0.0158        0.7227       0.0135   \n",
      "1  GroupKFold_by_stack    0.2867   0.2726        0.2944       0.2885   \n",
      "\n",
      "   roc_auc_mean  roc_auc_std  \n",
      "0        0.7808       0.0118  \n",
      "1           NaN          NaN  \n",
      "\n",
      "Final addendum summary preview:\n",
      "      section                              item            value\n",
      "0        Data                     Tracks (rows)           4263.0\n",
      "1        Data                     Features used             87.0\n",
      "2        Data                   Stacks (videos)             16.0\n",
      "3  Preprocess           Overall missingness (%)           29.119\n",
      "4  Preprocess        log1p-transformed features             38.0\n",
      "5      Labels                          WT count           2815.0\n",
      "6      Labels                         MUT count           1448.0\n",
      "7          CV         Best CV (by bal_acc_mean)  StratifiedKFold\n",
      "8          CV                 Best bal_acc_mean           0.7227\n",
      "9  Importance  Top feature (perm, bal_acc drop)            D_est\n"
     ]
    }
   ],
   "source": [
    "# Block 10: Final summary \n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PATH_METHOD = OUT_DIR / \"method_sanity_summary.csv\"\n",
    "PATH_PERM   = OUT_DIR / \"perm_importance_with_stability.csv\"\n",
    "PATH_CV     = OUT_DIR / \"cv_baseline_logreg_summary.csv\"\n",
    "\n",
    "def _load_csv(path: Path) -> pd.DataFrame:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def normalize_cv_summary(cv_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure we end up with a table that has:\n",
    "      - cv_strategy (string)\n",
    "      - acc_mean, acc_std, bal_acc_mean, bal_acc_std, roc_auc_mean, roc_auc_std (where available)\n",
    "    Handles cases where:\n",
    "      - cv_strategy was saved as index (e.g., first col is Unnamed: 0)\n",
    "      - columns are prefixed with '0_' and '1_' \n",
    "      - roc_auc columns missing for GroupKFold (common)\n",
    "    \"\"\"\n",
    "    df = cv_df.copy()\n",
    "\n",
    "    # If cv_strategy is missing but there's an unnamed first column, treat it as cv_strategy.\n",
    "    if \"cv_strategy\" not in df.columns:\n",
    "        unnamed = [c for c in df.columns if str(c).startswith(\"Unnamed\")]\n",
    "        if unnamed:\n",
    "            df = df.rename(columns={unnamed[0]: \"cv_strategy\"})\n",
    "        else:\n",
    "            # If still missing, but there is exactly 1 non-metric column, use it\n",
    "            non_numeric_cols = [c for c in df.columns if not pd.api.types.is_numeric_dtype(df[c])]\n",
    "            if len(non_numeric_cols) == 1:\n",
    "                df = df.rename(columns={non_numeric_cols[0]: \"cv_strategy\"})\n",
    "\n",
    "    # If still missing, create a placeholder based on row order (last resort)\n",
    "    if \"cv_strategy\" not in df.columns:\n",
    "        df[\"cv_strategy\"] = [f\"cv_{i}\" for i in range(len(df))]\n",
    "\n",
    "    # Case A: already in \"tidy\" format\n",
    "    tidy_cols = {\"acc_mean\",\"acc_std\",\"bal_acc_mean\",\"bal_acc_std\",\"roc_auc_mean\",\"roc_auc_std\"}\n",
    "    if any(c in df.columns for c in tidy_cols):\n",
    "        # Ensure all expected cols exist (fill with NaN if absent)\n",
    "        for c in tidy_cols:\n",
    "            if c not in df.columns:\n",
    "                df[c] = np.nan\n",
    "        return df[[\"cv_strategy\",\"acc_mean\",\"acc_std\",\"bal_acc_mean\",\"bal_acc_std\",\"roc_auc_mean\",\"roc_auc_std\"]]\n",
    "\n",
    "    # Case B:\n",
    "    # We'll detect prefixes \"<rowIndex>_<metric>\" and rebuild a tidy table.\n",
    "    split_cols = []\n",
    "    for c in df.columns:\n",
    "        if isinstance(c, str) and \"_\" in c and c.split(\"_\", 1)[0].isdigit():\n",
    "            split_cols.append(c)\n",
    "\n",
    "    if split_cols:\n",
    "        rows = []\n",
    "        for _, row in df.iterrows():\n",
    "            cv_name = str(row[\"cv_strategy\"])\n",
    "            # Gather all columns that begin with this row index if possible\n",
    "            out = {\"cv_strategy\": cv_name}\n",
    "            # Pull metrics by checking any column that endswith metric name\n",
    "            for metric in [\"acc_mean\",\"acc_std\",\"bal_acc_mean\",\"bal_acc_std\",\"roc_auc_mean\",\"roc_auc_std\"]:\n",
    "                candidates = [c for c in df.columns if c.endswith(f\"_{metric}\")]\n",
    "                if candidates:\n",
    "                    # If multiple, pick the first non-null value in this row\n",
    "                    val = np.nan\n",
    "                    for cc in candidates:\n",
    "                        if pd.notna(row[cc]):\n",
    "                            val = row[cc]\n",
    "                            break\n",
    "                    out[metric] = val\n",
    "                else:\n",
    "                    out[metric] = np.nan\n",
    "            rows.append(out)\n",
    "\n",
    "        tidy = pd.DataFrame(rows)\n",
    "        return tidy[[\"cv_strategy\",\"acc_mean\",\"acc_std\",\"bal_acc_mean\",\"bal_acc_std\",\"roc_auc_mean\",\"roc_auc_std\"]]\n",
    "\n",
    "    # Case C: unknown format — try to coerce numeric metric columns and keep them\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    keep = [\"cv_strategy\"] + numeric_cols\n",
    "    return df[keep]\n",
    "\n",
    "# ---- Load upstream outputs ----\n",
    "method_df = _load_csv(PATH_METHOD)\n",
    "perm_df   = _load_csv(PATH_PERM)\n",
    "cv_raw    = _load_csv(PATH_CV)\n",
    "cv_tidy   = normalize_cv_summary(cv_raw)\n",
    "\n",
    "# ---- Build a compact \"report-ready\" checklist table ----\n",
    "items = []\n",
    "\n",
    "# data stats\n",
    "def _pick(section, metric):\n",
    "    m = method_df[(method_df[\"section\"] == section) & (method_df[\"metric\"] == metric)]\n",
    "    return float(m[\"value\"].iloc[0]) if len(m) else np.nan\n",
    "\n",
    "items.append((\"Data\", \"Tracks (rows)\", _pick(\"data\",\"n_rows\")))\n",
    "items.append((\"Data\", \"Features used\", _pick(\"data\",\"n_features\")))\n",
    "items.append((\"Data\", \"Stacks (videos)\", _pick(\"data\",\"n_stacks\")))\n",
    "items.append((\"Preprocess\", \"Overall missingness (%)\", _pick(\"data\",\"pct_missing_overall_features\")))\n",
    "items.append((\"Preprocess\", \"log1p-transformed features\", _pick(\"transforms\",\"n_log1p_features\")))\n",
    "items.append((\"Labels\", \"WT count\", _pick(\"labels\",\"count_WT\")))\n",
    "items.append((\"Labels\", \"MUT count\", _pick(\"labels\",\"count_MUT\")))\n",
    "\n",
    "# CV best strategy (by balanced accuracy if available)\n",
    "best_cv_strategy = None\n",
    "best_bal = np.nan\n",
    "if \"bal_acc_mean\" in cv_tidy.columns:\n",
    "    tmp = cv_tidy.dropna(subset=[\"bal_acc_mean\"]).sort_values(\"bal_acc_mean\", ascending=False)\n",
    "    if len(tmp):\n",
    "        best_cv_strategy = str(tmp[\"cv_strategy\"].iloc[0])\n",
    "        best_bal = float(tmp[\"bal_acc_mean\"].iloc[0])\n",
    "\n",
    "items.append((\"CV\", \"Best CV (by bal_acc_mean)\", best_cv_strategy if best_cv_strategy is not None else \"NA\"))\n",
    "items.append((\"CV\", \"Best bal_acc_mean\", best_bal))\n",
    "\n",
    "# Permutation importance headline\n",
    "top_perm = perm_df.sort_values(\"mean_delta\", ascending=False).head(10)\n",
    "top_feat = str(top_perm[\"feature\"].iloc[0]) if len(top_perm) else \"NA\"\n",
    "items.append((\"Importance\", \"Top feature (perm, bal_acc drop)\", top_feat))\n",
    "\n",
    "summary_df = pd.DataFrame(items, columns=[\"section\",\"item\",\"value\"])\n",
    "out_path = OUT_DIR / \"final_addendum_summary.csv\"\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved: {out_path}\")\n",
    "print(\"\\nCV (tidy) preview:\")\n",
    "print(cv_tidy)\n",
    "\n",
    "print(\"\\nFinal addendum summary preview:\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb7f3355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved method justification checklist to:\n",
      "\\Users\\asus\\Downloads\\MSc Final Project Stuff\\outputs_addendum\\method_justification_checklist.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decision</th>\n",
       "      <th>why_chosen</th>\n",
       "      <th>alternatives_considered</th>\n",
       "      <th>assumptions</th>\n",
       "      <th>limitations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Log1p transformation of skewed features</td>\n",
       "      <td>Many per-track metrics showed strong positive ...</td>\n",
       "      <td>No transform; Box–Cox (requires strictly posit...</td>\n",
       "      <td>Monotonic transform preserves rank ordering</td>\n",
       "      <td>Does not guarantee normality; interpretation s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Use of nonparametric tests (e.g. Mann–Whitney ...</td>\n",
       "      <td>Preliminary inspection showed non-Gaussian dis...</td>\n",
       "      <td>Two-sample t-test; Welch’s t-test</td>\n",
       "      <td>Independence; ordinal comparability</td>\n",
       "      <td>Sensitive only to monotone differences; reduce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Group-aware cross-validation by video stack</td>\n",
       "      <td>Tracks from the same video are not statistical...</td>\n",
       "      <td>Standard StratifiedKFold</td>\n",
       "      <td>Stack-level grouping captures dominant depende...</td>\n",
       "      <td>Higher variance; fewer effective folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Permutation importance under group-aware CV</td>\n",
       "      <td>Permutation importance evaluates feature contr...</td>\n",
       "      <td>Model coefficients; impurity-based importance</td>\n",
       "      <td>Feature perturbation reflects information content</td>\n",
       "      <td>Unstable with correlated features; fold-depend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic regression as baseline classifier</td>\n",
       "      <td>Provides a transparent linear baseline with in...</td>\n",
       "      <td>Tree ensembles; kernel classifiers</td>\n",
       "      <td>Approximate linear separability in feature space</td>\n",
       "      <td>Limited capacity for complex nonlinear interac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            decision  \\\n",
       "0            Log1p transformation of skewed features   \n",
       "1  Use of nonparametric tests (e.g. Mann–Whitney ...   \n",
       "2        Group-aware cross-validation by video stack   \n",
       "3        Permutation importance under group-aware CV   \n",
       "4         Logistic regression as baseline classifier   \n",
       "\n",
       "                                          why_chosen  \\\n",
       "0  Many per-track metrics showed strong positive ...   \n",
       "1  Preliminary inspection showed non-Gaussian dis...   \n",
       "2  Tracks from the same video are not statistical...   \n",
       "3  Permutation importance evaluates feature contr...   \n",
       "4  Provides a transparent linear baseline with in...   \n",
       "\n",
       "                             alternatives_considered  \\\n",
       "0  No transform; Box–Cox (requires strictly posit...   \n",
       "1                  Two-sample t-test; Welch’s t-test   \n",
       "2                           Standard StratifiedKFold   \n",
       "3      Model coefficients; impurity-based importance   \n",
       "4                 Tree ensembles; kernel classifiers   \n",
       "\n",
       "                                         assumptions  \\\n",
       "0        Monotonic transform preserves rank ordering   \n",
       "1                Independence; ordinal comparability   \n",
       "2  Stack-level grouping captures dominant depende...   \n",
       "3  Feature perturbation reflects information content   \n",
       "4   Approximate linear separability in feature space   \n",
       "\n",
       "                                         limitations  \n",
       "0  Does not guarantee normality; interpretation s...  \n",
       "1  Sensitive only to monotone differences; reduce...  \n",
       "2             Higher variance; fewer effective folds  \n",
       "3  Unstable with correlated features; fold-depend...  \n",
       "4  Limited capacity for complex nonlinear interac...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Block 11: Method justification checklist\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "justification_rows = [\n",
    "\n",
    "    # Data characteristics\n",
    "    {\n",
    "        \"decision\": \"Log1p transformation of skewed features\",\n",
    "        \"why_chosen\": (\n",
    "            \"Many per-track metrics showed strong positive skew and heavy tails. \"\n",
    "            \"Log1p stabilizes variance while preserving zero values, improving \"\n",
    "            \"comparability and interpretability.\"\n",
    "        ),\n",
    "        \"alternatives_considered\": \"No transform; Box–Cox (requires strictly positive data)\",\n",
    "        \"assumptions\": \"Monotonic transform preserves rank ordering\",\n",
    "        \"limitations\": \"Does not guarantee normality; interpretation shifts to log scale\"\n",
    "    },\n",
    "\n",
    "    # Statistical testing\n",
    "    {\n",
    "        \"decision\": \"Use of nonparametric tests (e.g. Mann–Whitney U) for group comparisons\",\n",
    "        \"why_chosen\": (\n",
    "            \"Preliminary inspection showed non-Gaussian distributions and unequal variances. \"\n",
    "            \"Rank-based tests avoid parametric assumptions under these conditions.\"\n",
    "        ),\n",
    "        \"alternatives_considered\": \"Two-sample t-test; Welch’s t-test\",\n",
    "        \"assumptions\": \"Independence; ordinal comparability\",\n",
    "        \"limitations\": \"Sensitive only to monotone differences; reduced power under normality\"\n",
    "    },\n",
    "\n",
    "    # Model evaluation\n",
    "    {\n",
    "        \"decision\": \"Group-aware cross-validation by video stack\",\n",
    "        \"why_chosen\": (\n",
    "            \"Tracks from the same video are not statistically independent. \"\n",
    "            \"GroupKFold prevents information leakage across folds.\"\n",
    "        ),\n",
    "        \"alternatives_considered\": \"Standard StratifiedKFold\",\n",
    "        \"assumptions\": \"Stack-level grouping captures dominant dependence structure\",\n",
    "        \"limitations\": \"Higher variance; fewer effective folds\"\n",
    "    },\n",
    "\n",
    "    # Feature importance\n",
    "    {\n",
    "        \"decision\": \"Permutation importance under group-aware CV\",\n",
    "        \"why_chosen\": (\n",
    "            \"Permutation importance evaluates feature contribution without assuming \"\n",
    "            \"linearity and reflects performance sensitivity under realistic validation.\"\n",
    "        ),\n",
    "        \"alternatives_considered\": \"Model coefficients; impurity-based importance\",\n",
    "        \"assumptions\": \"Feature perturbation reflects information content\",\n",
    "        \"limitations\": \"Unstable with correlated features; fold-dependent variability\"\n",
    "    },\n",
    "\n",
    "    # Modeling choice\n",
    "    {\n",
    "        \"decision\": \"Logistic regression as baseline classifier\",\n",
    "        \"why_chosen\": (\n",
    "            \"Provides a transparent linear baseline with interpretable coefficients \"\n",
    "            \"and stable optimization under high-dimensional features.\"\n",
    "        ),\n",
    "        \"alternatives_considered\": \"Tree ensembles; kernel classifiers\",\n",
    "        \"assumptions\": \"Approximate linear separability in feature space\",\n",
    "        \"limitations\": \"Limited capacity for complex nonlinear interactions\"\n",
    "    }\n",
    "]\n",
    "\n",
    "justification_df = pd.DataFrame(justification_rows)\n",
    "\n",
    "out_path = OUT_DIR / \"method_justification_checklist.csv\"\n",
    "justification_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved method justification checklist to:\\n{out_path}\")\n",
    "display(justification_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (musc_clean)",
   "language": "python",
   "name": "musc_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
